nohup: ignoring input
 #######################
 ##The 01th experiment##
 #######################
 learning rate: 0.1
 Weight decay: 0.005
Construct model vgg13_bn
Epoch 0001, val loss 22.2139, val loss 21.5442
Epoch 0002, val loss 22.1418, val loss 21.4847
Epoch 0003, val loss 24.9701, val loss 22.4681
Epoch 0004, val loss 89.4731, val loss 92.8416
Epoch 0005, val loss 40.7873, val loss 40.4827
Epoch 0006, val loss 174.5186, val loss 174.6664
Epoch 0007, val loss 174.5186, val loss 174.6664
Epoch 0008, val loss 25.4741, val loss 24.8822
Epoch 0009, val loss 23.5955, val loss 23.1280
Epoch 0010, val loss 24.3291, val loss 24.1945
Epoch 0011, val loss 23.1273, val loss 22.5778
Epoch 0012, val loss 22.9689, val loss 22.5313
Optimization Finished!
Total time elapsed: 191.1801s
Loading 2th epoch
Test loss 21.4847

 #######################
 ##The 02th experiment##
 #######################
 learning rate: 0.1
 Weight decay: 0.001
Construct model vgg13_bn
Epoch 0001, val loss 22.4426, val loss 21.7903
Epoch 0002, val loss 174.5186, val loss 174.6664
Epoch 0003, val loss 24.5626, val loss 23.7397
Epoch 0004, val loss 22.8891, val loss 22.1442
Epoch 0005, val loss 21.8035, val loss 21.0623
Epoch 0006, val loss 21.7230, val loss 20.9681
Epoch 0007, val loss 21.7321, val loss 20.9885
Epoch 0008, val loss 21.6037, val loss 20.8874
Epoch 0009, val loss 21.7251, val loss 20.9844
Epoch 0010, val loss 21.6216, val loss 20.9290
Epoch 0011, val loss 21.5360, val loss 20.9511
Epoch 0012, val loss 21.4926, val loss 20.7677
Epoch 0013, val loss 21.4883, val loss 20.7552
Epoch 0014, val loss 21.5342, val loss 20.7913
Epoch 0015, val loss 21.6016, val loss 20.9032
Epoch 0016, val loss 21.6254, val loss 20.9495
Epoch 0017, val loss 21.5513, val loss 20.8567
Epoch 0018, val loss 21.4751, val loss 20.7871
Epoch 0019, val loss 21.5213, val loss 20.8412
Epoch 0020, val loss 21.6531, val loss 20.9776
Epoch 0021, val loss 21.4327, val loss 20.8246
Epoch 0022, val loss 21.3442, val loss 20.6408
Epoch 0023, val loss 21.2989, val loss 20.6510
Epoch 0024, val loss 21.3302, val loss 20.6468
Epoch 0025, val loss 21.3807, val loss 20.7261
Epoch 0026, val loss 21.3175, val loss 20.6532
Epoch 0027, val loss 21.3132, val loss 20.6951
Epoch 0028, val loss 21.4002, val loss 20.7555
Epoch 0029, val loss 21.6458, val loss 20.9606
Epoch 0030, val loss 21.4405, val loss 20.7469
Epoch 0031, val loss 21.4399, val loss 20.8037
Epoch 0032, val loss 21.4047, val loss 20.7875
Epoch 0033, val loss 21.6036, val loss 20.8967
Optimization Finished!
Total time elapsed: 528.2056s
Loading 23th epoch
Test loss 20.6510

 #######################
 ##The 03th experiment##
 #######################
 learning rate: 0.01
 Weight decay: 0.005
Construct model vgg13_bn
Epoch 0001, val loss 22.6639, val loss 22.0167
Epoch 0002, val loss 20.8797, val loss 20.2710
Epoch 0003, val loss 17.6726, val loss 17.2695
Epoch 0004, val loss 15.6325, val loss 15.4260
Epoch 0005, val loss 14.9817, val loss 14.8128
Epoch 0006, val loss 14.7239, val loss 14.5418
Epoch 0007, val loss 14.0272, val loss 13.9232
Epoch 0008, val loss 13.8158, val loss 13.6627
Epoch 0009, val loss 13.6252, val loss 13.4757
Epoch 0010, val loss 13.0640, val loss 12.8823
Epoch 0011, val loss 13.0782, val loss 12.8822
Epoch 0012, val loss 12.8362, val loss 12.6893
Epoch 0013, val loss 12.8901, val loss 12.7131
Epoch 0014, val loss 12.6416, val loss 12.4502
Epoch 0015, val loss 12.7525, val loss 12.3555
Epoch 0016, val loss 12.7453, val loss 12.5169
Epoch 0017, val loss 12.3710, val loss 12.0575
Epoch 0018, val loss 12.1584, val loss 11.9248
Epoch 0019, val loss 12.3902, val loss 12.1022
Epoch 0020, val loss 12.2381, val loss 11.9669
Epoch 0021, val loss 12.0827, val loss 11.8729
Epoch 0022, val loss 11.8672, val loss 11.6295
Epoch 0023, val loss 12.0117, val loss 11.6853
Epoch 0024, val loss 11.6294, val loss 11.3461
Epoch 0025, val loss 11.7732, val loss 11.4962
Epoch 0026, val loss 11.8252, val loss 11.5505
Epoch 0027, val loss 11.8203, val loss 11.5188
Epoch 0028, val loss 11.5521, val loss 11.3107
Epoch 0029, val loss 11.8134, val loss 11.4577
Epoch 0030, val loss 11.6648, val loss 11.3698
Epoch 0031, val loss 11.7512, val loss 11.4340
Epoch 0032, val loss 11.5975, val loss 11.2851
Epoch 0033, val loss 11.5260, val loss 11.1970
Epoch 0034, val loss 11.3986, val loss 11.1753
Epoch 0035, val loss 11.5007, val loss 11.2242
Epoch 0036, val loss 11.4769, val loss 11.1494
Epoch 0037, val loss 11.2241, val loss 10.9471
Epoch 0038, val loss 11.1965, val loss 10.9737
Epoch 0039, val loss 11.3568, val loss 11.0815
Epoch 0040, val loss 11.3940, val loss 11.0543
Epoch 0041, val loss 11.6759, val loss 11.4153
Epoch 0042, val loss 11.2641, val loss 10.9818
Epoch 0043, val loss 11.3893, val loss 11.1243
Epoch 0044, val loss 11.2577, val loss 10.9932
Epoch 0045, val loss 11.4266, val loss 11.0967
Epoch 0046, val loss 11.4852, val loss 11.1773
Epoch 0047, val loss 11.1633, val loss 10.8616
Epoch 0048, val loss 11.1770, val loss 10.9422
Epoch 0049, val loss 11.1583, val loss 10.8443
Epoch 0050, val loss 10.9481, val loss 10.6374
Epoch 0051, val loss 11.0869, val loss 10.8382
Epoch 0052, val loss 11.1928, val loss 10.8635
Epoch 0053, val loss 11.0682, val loss 10.7369
Epoch 0054, val loss 11.0550, val loss 10.7664
Epoch 0055, val loss 11.0617, val loss 10.7637
Epoch 0056, val loss 11.1870, val loss 10.9673
Epoch 0057, val loss 10.8719, val loss 10.6063
Epoch 0058, val loss 11.0536, val loss 10.7291
Epoch 0059, val loss 11.1383, val loss 10.8034
Epoch 0060, val loss 10.9010, val loss 10.5966
Epoch 0061, val loss 10.9825, val loss 10.6832
Epoch 0062, val loss 10.7168, val loss 10.4115
Epoch 0063, val loss 10.9208, val loss 10.5274
Epoch 0064, val loss 11.0578, val loss 10.7738
Epoch 0065, val loss 10.9951, val loss 10.6547
Epoch 0066, val loss 10.8202, val loss 10.5299
Epoch 0067, val loss 10.8933, val loss 10.6026
Epoch 0068, val loss 10.8605, val loss 10.5416
Epoch 0069, val loss 10.8044, val loss 10.5283
Epoch 0070, val loss 10.6750, val loss 10.3639
Epoch 0071, val loss 10.7570, val loss 10.4100
Epoch 0072, val loss 10.7857, val loss 10.4685
Epoch 0073, val loss 10.8793, val loss 10.5829
Epoch 0074, val loss 10.8739, val loss 10.5585
Epoch 0075, val loss 10.6811, val loss 10.4110
Epoch 0076, val loss 10.6056, val loss 10.2797
Epoch 0077, val loss 10.5982, val loss 10.2877
Epoch 0078, val loss 10.7067, val loss 10.3663
Epoch 0079, val loss 10.7457, val loss 10.4376
Epoch 0080, val loss 10.8471, val loss 10.4736
Epoch 0081, val loss 10.7135, val loss 10.4132
Epoch 0082, val loss 10.6947, val loss 10.3584
Epoch 0083, val loss 10.6552, val loss 10.3228
Epoch 0084, val loss 10.4443, val loss 10.0844
Epoch 0085, val loss 10.6019, val loss 10.2560
Epoch 0086, val loss 10.7823, val loss 10.5200
Epoch 0087, val loss 10.9767, val loss 10.6151
Epoch 0088, val loss 10.4985, val loss 10.1802
Epoch 0089, val loss 10.5031, val loss 10.1389
Epoch 0090, val loss 10.5619, val loss 10.2633
Epoch 0091, val loss 10.3199, val loss 9.9864
Epoch 0092, val loss 10.6507, val loss 10.3282
Epoch 0093, val loss 10.5099, val loss 10.2117
Epoch 0094, val loss 10.4606, val loss 10.1430
Epoch 0095, val loss 10.5698, val loss 10.2910
Epoch 0096, val loss 11.0583, val loss 10.7439
Epoch 0097, val loss 10.6948, val loss 10.4266
Epoch 0098, val loss 10.4222, val loss 10.1127
Epoch 0099, val loss 10.4897, val loss 10.1713
Epoch 0100, val loss 10.4860, val loss 10.1516
Epoch 0101, val loss 10.4661, val loss 10.1517
Optimization Finished!
Total time elapsed: 1634.4018s
Loading 91th epoch
Test loss 9.9864

 #######################
 ##The 04th experiment##
 #######################
 learning rate: 0.01
 Weight decay: 0.001
Construct model vgg13_bn
Epoch 0001, val loss 22.2628, val loss 21.6271
Epoch 0002, val loss 21.3507, val loss 20.6937
Epoch 0003, val loss 17.3794, val loss 16.9770
Epoch 0004, val loss 15.9112, val loss 15.5461
Epoch 0005, val loss 15.0770, val loss 14.9438
Epoch 0006, val loss 14.4414, val loss 14.3518
Epoch 0007, val loss 13.5108, val loss 13.3464
Epoch 0008, val loss 13.2848, val loss 13.0282
Epoch 0009, val loss 13.4106, val loss 13.1650
Epoch 0010, val loss 12.7581, val loss 12.4188
Epoch 0011, val loss 12.0335, val loss 11.8110
Epoch 0012, val loss 11.9587, val loss 11.7184
Epoch 0013, val loss 11.6389, val loss 11.3759
Epoch 0014, val loss 11.5116, val loss 11.1336
Epoch 0015, val loss 11.2353, val loss 10.9105
Epoch 0016, val loss 11.7418, val loss 11.1830
Epoch 0017, val loss 11.1089, val loss 10.8051
Epoch 0018, val loss 10.0203, val loss 9.7864
Epoch 0019, val loss 9.8123, val loss 9.8404
Epoch 0020, val loss 8.8147, val loss 8.8706
Epoch 0021, val loss 9.3485, val loss 9.1659
Epoch 0022, val loss 8.6609, val loss 8.5697
Epoch 0023, val loss 8.6390, val loss 8.4918
Epoch 0024, val loss 8.1631, val loss 8.2123
Epoch 0025, val loss 8.3419, val loss 8.2991
Epoch 0026, val loss 7.5997, val loss 7.5559
Epoch 0027, val loss 7.8147, val loss 7.6919
Epoch 0028, val loss 7.8065, val loss 7.7304
Epoch 0029, val loss 7.3020, val loss 7.3410
Epoch 0030, val loss 7.5738, val loss 7.5674
Epoch 0031, val loss 7.2368, val loss 7.1394
Epoch 0032, val loss 7.4010, val loss 7.3279
Epoch 0033, val loss 7.1306, val loss 7.0410
Epoch 0034, val loss 7.0631, val loss 7.0238
Epoch 0035, val loss 7.0021, val loss 6.9993
Epoch 0036, val loss 7.0255, val loss 6.9329
Epoch 0037, val loss 6.8610, val loss 6.8500
Epoch 0038, val loss 6.6676, val loss 6.6102
Epoch 0039, val loss 6.7559, val loss 6.7038
Epoch 0040, val loss 6.4518, val loss 6.5042
Epoch 0041, val loss 6.3761, val loss 6.2570
Epoch 0042, val loss 6.5967, val loss 6.5442
Epoch 0043, val loss 6.3998, val loss 6.2634
Epoch 0044, val loss 6.4961, val loss 6.4089
Epoch 0045, val loss 6.5298, val loss 6.4638
Epoch 0046, val loss 6.3404, val loss 6.2954
Epoch 0047, val loss 6.3520, val loss 6.3001
Epoch 0048, val loss 6.4558, val loss 6.2730
Epoch 0049, val loss 6.0803, val loss 5.9647
Epoch 0050, val loss 6.1806, val loss 6.0545
Epoch 0051, val loss 5.9504, val loss 5.9107
Epoch 0052, val loss 5.8694, val loss 5.7794
Epoch 0053, val loss 5.6565, val loss 5.5203
Epoch 0054, val loss 5.6412, val loss 5.5572
Epoch 0055, val loss 5.6548, val loss 5.5813
Epoch 0056, val loss 5.7899, val loss 5.7492
Epoch 0057, val loss 5.9174, val loss 5.8310
Epoch 0058, val loss 5.7590, val loss 5.6923
Epoch 0059, val loss 5.5278, val loss 5.4088
Epoch 0060, val loss 5.3106, val loss 5.2334
Epoch 0061, val loss 5.5362, val loss 5.4169
Epoch 0062, val loss 5.7230, val loss 5.6954
Epoch 0063, val loss 5.3893, val loss 5.3886
Epoch 0064, val loss 5.4464, val loss 5.3509
Epoch 0065, val loss 5.3025, val loss 5.2594
Epoch 0066, val loss 5.5779, val loss 5.5132
Epoch 0067, val loss 5.3732, val loss 5.2613
Epoch 0068, val loss 5.4257, val loss 5.3190
Epoch 0069, val loss 5.3500, val loss 5.2466
Epoch 0070, val loss 5.1980, val loss 5.1104
Epoch 0071, val loss 5.3287, val loss 5.2615
Epoch 0072, val loss 5.2621, val loss 5.2263
Epoch 0073, val loss 5.2589, val loss 5.1870
Epoch 0074, val loss 5.3483, val loss 5.2493
Epoch 0075, val loss 5.2372, val loss 5.1159
Epoch 0076, val loss 4.8801, val loss 4.7784
Epoch 0077, val loss 5.0219, val loss 4.9430
Epoch 0078, val loss 5.0400, val loss 4.8897
Epoch 0079, val loss 5.4100, val loss 5.3050
Epoch 0080, val loss 5.2610, val loss 5.2411
Epoch 0081, val loss 5.0405, val loss 5.0060
Epoch 0082, val loss 4.8565, val loss 4.8753
Epoch 0083, val loss 4.8414, val loss 4.7855
Epoch 0084, val loss 4.8159, val loss 4.7742
Epoch 0085, val loss 4.9007, val loss 4.8612
Epoch 0086, val loss 4.7409, val loss 4.6970
Epoch 0087, val loss 4.8195, val loss 4.7672
Epoch 0088, val loss 4.6889, val loss 4.6183
Epoch 0089, val loss 4.7620, val loss 4.6995
Epoch 0090, val loss 4.7124, val loss 4.6160
Epoch 0091, val loss 4.6244, val loss 4.6012
Epoch 0092, val loss 4.8458, val loss 4.7733
Epoch 0093, val loss 4.8715, val loss 4.7346
Epoch 0094, val loss 4.7226, val loss 4.6344
Epoch 0095, val loss 4.6330, val loss 4.5544
Epoch 0096, val loss 4.4850, val loss 4.4680
Epoch 0097, val loss 4.4745, val loss 4.4040
Epoch 0098, val loss 4.5333, val loss 4.5234
Epoch 0099, val loss 4.6217, val loss 4.4782
Epoch 0100, val loss 4.5750, val loss 4.5062
Epoch 0101, val loss 4.7237, val loss 4.6721
Epoch 0102, val loss 4.4351, val loss 4.3006
Epoch 0103, val loss 4.5142, val loss 4.4209
Epoch 0104, val loss 4.6882, val loss 4.6690
Epoch 0105, val loss 4.4809, val loss 4.4637
Epoch 0106, val loss 4.6528, val loss 4.5589
Epoch 0107, val loss 4.5645, val loss 4.4711
Epoch 0108, val loss 4.3527, val loss 4.2868
Epoch 0109, val loss 4.3131, val loss 4.2745
Epoch 0110, val loss 4.1923, val loss 4.1275
Epoch 0111, val loss 4.2067, val loss 4.1168
Epoch 0112, val loss 4.1869, val loss 4.0990
Epoch 0113, val loss 4.3728, val loss 4.2366
Epoch 0114, val loss 4.4323, val loss 4.3452
Epoch 0115, val loss 4.7375, val loss 4.6214
Epoch 0116, val loss 4.5435, val loss 4.4761
Epoch 0117, val loss 4.1908, val loss 4.0838
Epoch 0118, val loss 4.3127, val loss 4.1945
Epoch 0119, val loss 4.4351, val loss 4.2688
Epoch 0120, val loss 4.4076, val loss 4.2711
Epoch 0121, val loss 4.3414, val loss 4.2662
Epoch 0122, val loss 4.2154, val loss 4.1406
Optimization Finished!
Total time elapsed: 1973.9262s
Loading 112th epoch
Test loss 4.0990

 #######################
 ##The 05th experiment##
 #######################
 learning rate: 0.001
 Weight decay: 0.005
Construct model vgg13_bn
Epoch 0001, val loss 20.1284, val loss 19.7207
Epoch 0002, val loss 14.9075, val loss 14.9121
Epoch 0003, val loss 13.8065, val loss 13.7916
Epoch 0004, val loss 12.9671, val loss 13.0272
Epoch 0005, val loss 12.4369, val loss 12.5152
Epoch 0006, val loss 12.3830, val loss 12.3962
Epoch 0007, val loss 12.0121, val loss 11.9828
Epoch 0008, val loss 11.5984, val loss 11.7124
Epoch 0009, val loss 11.4205, val loss 11.4734
Epoch 0010, val loss 11.0835, val loss 11.1273
Epoch 0011, val loss 11.0461, val loss 11.0350
Epoch 0012, val loss 10.8067, val loss 10.7960
Epoch 0013, val loss 10.8454, val loss 10.8639
Epoch 0014, val loss 10.5065, val loss 10.5584
Epoch 0015, val loss 10.4433, val loss 10.4529
Epoch 0016, val loss 10.1894, val loss 10.2418
Epoch 0017, val loss 10.2221, val loss 10.2773
Epoch 0018, val loss 10.1998, val loss 10.2780
Epoch 0019, val loss 10.0518, val loss 10.0700
Epoch 0020, val loss 9.9869, val loss 10.0924
Epoch 0021, val loss 10.0371, val loss 10.0657
Epoch 0022, val loss 9.9791, val loss 10.0080
Epoch 0023, val loss 9.8994, val loss 9.9631
Epoch 0024, val loss 9.9323, val loss 9.9575
Epoch 0025, val loss 9.7482, val loss 9.7747
Epoch 0026, val loss 9.6889, val loss 9.7629
Epoch 0027, val loss 9.5302, val loss 9.5147
Epoch 0028, val loss 9.6990, val loss 9.7456
Epoch 0029, val loss 9.3952, val loss 9.3863
Epoch 0030, val loss 9.4823, val loss 9.5037
Epoch 0031, val loss 9.3398, val loss 9.3894
Epoch 0032, val loss 9.3836, val loss 9.4017
Epoch 0033, val loss 9.3692, val loss 9.4568
Epoch 0034, val loss 9.5294, val loss 9.5958
Epoch 0035, val loss 9.2969, val loss 9.3364
Epoch 0036, val loss 9.4099, val loss 9.3905
Epoch 0037, val loss 9.2794, val loss 9.2872
Epoch 0038, val loss 9.2679, val loss 9.2877
Epoch 0039, val loss 9.1613, val loss 9.1898
Epoch 0040, val loss 9.0651, val loss 9.1008
Epoch 0041, val loss 9.0479, val loss 9.0836
Epoch 0042, val loss 9.2377, val loss 9.2687
Epoch 0043, val loss 9.0037, val loss 8.9871
Epoch 0044, val loss 8.9788, val loss 8.9730
Epoch 0045, val loss 9.1727, val loss 9.2135
Epoch 0046, val loss 8.9074, val loss 8.9135
Epoch 0047, val loss 9.0892, val loss 9.0441
Epoch 0048, val loss 8.9829, val loss 8.9832
Epoch 0049, val loss 9.0281, val loss 9.0026
Epoch 0050, val loss 8.9407, val loss 8.9169
Epoch 0051, val loss 8.8969, val loss 8.9047
Epoch 0052, val loss 8.7611, val loss 8.7624
Epoch 0053, val loss 8.8535, val loss 8.7989
Epoch 0054, val loss 8.9985, val loss 8.9917
Epoch 0055, val loss 8.8552, val loss 8.8319
Epoch 0056, val loss 8.8435, val loss 8.8057
Epoch 0057, val loss 8.7557, val loss 8.7613
Epoch 0058, val loss 8.7593, val loss 8.7570
Epoch 0059, val loss 8.7223, val loss 8.7171
Epoch 0060, val loss 8.6792, val loss 8.7010
Epoch 0061, val loss 8.7483, val loss 8.7617
Epoch 0062, val loss 8.6076, val loss 8.6836
Epoch 0063, val loss 8.6096, val loss 8.6479
Epoch 0064, val loss 8.7512, val loss 8.7548
Epoch 0065, val loss 8.7899, val loss 8.7613
Epoch 0066, val loss 8.7592, val loss 8.7435
Epoch 0067, val loss 8.6554, val loss 8.6615
Epoch 0068, val loss 8.6630, val loss 8.7128
Epoch 0069, val loss 8.5772, val loss 8.5594
Epoch 0070, val loss 8.7350, val loss 8.7236
Epoch 0071, val loss 8.6491, val loss 8.6246
Epoch 0072, val loss 8.6073, val loss 8.5614
Epoch 0073, val loss 8.6735, val loss 8.6342
Epoch 0074, val loss 8.6709, val loss 8.6239
Epoch 0075, val loss 8.6077, val loss 8.5594
Epoch 0076, val loss 8.7165, val loss 8.7411
Epoch 0077, val loss 8.7348, val loss 8.7250
Epoch 0078, val loss 8.7063, val loss 8.6729
Epoch 0079, val loss 8.7639, val loss 8.7053
Optimization Finished!
Total time elapsed: 1283.9864s
Loading 69th epoch
Test loss 8.5594

 #######################
 ##The 06th experiment##
 #######################
 learning rate: 0.001
 Weight decay: 0.001
Construct model vgg13_bn
Epoch 0001, val loss 20.3070, val loss 19.9213
Epoch 0002, val loss 14.7416, val loss 14.6151
Epoch 0003, val loss 13.2068, val loss 13.2385
Epoch 0004, val loss 12.1723, val loss 12.2299
Epoch 0005, val loss 11.1388, val loss 11.2368
Epoch 0006, val loss 10.3561, val loss 10.4553
Epoch 0007, val loss 10.6086, val loss 10.5571
Epoch 0008, val loss 10.1810, val loss 10.1573
Epoch 0009, val loss 9.4387, val loss 9.5168
Epoch 0010, val loss 9.2989, val loss 9.3135
Epoch 0011, val loss 9.3145, val loss 9.3586
Epoch 0012, val loss 9.0910, val loss 9.0491
Epoch 0013, val loss 8.9027, val loss 8.8774
Epoch 0014, val loss 8.6333, val loss 8.6462
Epoch 0015, val loss 8.6708, val loss 8.6646
Epoch 0016, val loss 8.1786, val loss 8.1524
Epoch 0017, val loss 8.2269, val loss 8.2017
Epoch 0018, val loss 7.9710, val loss 8.0721
Epoch 0019, val loss 7.6702, val loss 7.6228
Epoch 0020, val loss 8.0324, val loss 8.0207
Epoch 0021, val loss 7.8533, val loss 7.8354
Epoch 0022, val loss 7.7447, val loss 7.7714
Epoch 0023, val loss 7.4214, val loss 7.4052
Epoch 0024, val loss 7.3238, val loss 7.3260
Epoch 0025, val loss 7.3262, val loss 7.3578
Epoch 0026, val loss 6.8018, val loss 6.7932
Epoch 0027, val loss 6.8642, val loss 6.8070
Epoch 0028, val loss 6.8019, val loss 6.8480
Epoch 0029, val loss 6.7634, val loss 6.6804
Epoch 0030, val loss 6.5665, val loss 6.6576
Epoch 0031, val loss 6.7984, val loss 6.7570
Epoch 0032, val loss 6.7504, val loss 6.6088
Epoch 0033, val loss 6.5510, val loss 6.4817
Epoch 0034, val loss 6.3805, val loss 6.3561
Epoch 0035, val loss 6.4587, val loss 6.4708
Epoch 0036, val loss 6.5046, val loss 6.4135
Epoch 0037, val loss 6.3889, val loss 6.4184
Epoch 0038, val loss 6.2539, val loss 6.2826
Epoch 0039, val loss 6.0013, val loss 5.9612
Epoch 0040, val loss 6.1777, val loss 6.0527
Epoch 0041, val loss 5.9273, val loss 5.8545
Epoch 0042, val loss 5.9498, val loss 5.9158
Epoch 0043, val loss 6.1449, val loss 6.0844
Epoch 0044, val loss 5.9038, val loss 5.8444
Epoch 0045, val loss 5.8849, val loss 5.8026
Epoch 0046, val loss 5.8316, val loss 5.7741
Epoch 0047, val loss 5.8510, val loss 5.7623
Epoch 0048, val loss 5.9401, val loss 5.9138
Epoch 0049, val loss 5.9768, val loss 5.9443
Epoch 0050, val loss 5.8745, val loss 5.7772
Epoch 0051, val loss 5.8096, val loss 5.7765
Epoch 0052, val loss 5.7438, val loss 5.6604
Epoch 0053, val loss 5.8922, val loss 5.7342
Epoch 0054, val loss 5.7341, val loss 5.6915
Epoch 0055, val loss 5.4395, val loss 5.3140
Epoch 0056, val loss 5.6532, val loss 5.5039
Epoch 0057, val loss 5.5760, val loss 5.4797
Epoch 0058, val loss 5.5604, val loss 5.4834
Epoch 0059, val loss 5.6922, val loss 5.5272
Epoch 0060, val loss 5.3738, val loss 5.2902
Epoch 0061, val loss 5.4612, val loss 5.3549
Epoch 0062, val loss 5.4583, val loss 5.3556
Epoch 0063, val loss 5.4526, val loss 5.3844
Epoch 0064, val loss 5.4152, val loss 5.2887
Epoch 0065, val loss 5.3260, val loss 5.2368
Epoch 0066, val loss 5.2233, val loss 5.1201
Epoch 0067, val loss 5.5379, val loss 5.4926
Epoch 0068, val loss 5.3745, val loss 5.2977
Epoch 0069, val loss 5.1640, val loss 5.0444
Epoch 0070, val loss 5.1391, val loss 5.0355
Epoch 0071, val loss 5.2040, val loss 5.1027
Epoch 0072, val loss 5.0280, val loss 4.9126
Epoch 0073, val loss 5.2078, val loss 5.1124
Epoch 0074, val loss 5.0740, val loss 4.9780
Epoch 0075, val loss 5.1100, val loss 4.9954
Epoch 0076, val loss 5.1564, val loss 5.0926
Epoch 0077, val loss 5.1766, val loss 5.0550
Epoch 0078, val loss 5.1563, val loss 5.0722
Epoch 0079, val loss 5.0959, val loss 4.9527
Epoch 0080, val loss 4.9829, val loss 4.8636
Epoch 0081, val loss 4.9940, val loss 4.8357
Epoch 0082, val loss 5.1435, val loss 5.0700
Epoch 0083, val loss 4.9243, val loss 4.7736
Epoch 0084, val loss 4.7444, val loss 4.6772
Epoch 0085, val loss 4.8517, val loss 4.7466
Epoch 0086, val loss 4.8997, val loss 4.7931
Epoch 0087, val loss 4.8484, val loss 4.7540
Epoch 0088, val loss 5.0708, val loss 4.9542
Epoch 0089, val loss 4.8123, val loss 4.7361
Epoch 0090, val loss 4.9394, val loss 4.8523
Epoch 0091, val loss 4.9940, val loss 4.8752
Epoch 0092, val loss 4.8029, val loss 4.7295
Epoch 0093, val loss 4.7697, val loss 4.6654
Epoch 0094, val loss 4.6237, val loss 4.5399
Epoch 0095, val loss 4.7069, val loss 4.6345
Epoch 0096, val loss 4.6810, val loss 4.5609
Epoch 0097, val loss 4.6688, val loss 4.5771
Epoch 0098, val loss 4.6580, val loss 4.5980
Epoch 0099, val loss 4.8289, val loss 4.7207
Epoch 0100, val loss 4.8471, val loss 4.7564
Epoch 0101, val loss 4.6777, val loss 4.5495
Epoch 0102, val loss 4.5974, val loss 4.4857
Epoch 0103, val loss 4.6460, val loss 4.5408
Epoch 0104, val loss 4.6064, val loss 4.5454
Epoch 0105, val loss 4.6140, val loss 4.5073
Epoch 0106, val loss 4.5819, val loss 4.4477
Epoch 0107, val loss 4.5158, val loss 4.4150
Epoch 0108, val loss 4.5300, val loss 4.4545
Epoch 0109, val loss 4.4944, val loss 4.3732
Epoch 0110, val loss 4.4721, val loss 4.4109
Epoch 0111, val loss 4.3933, val loss 4.3216
Epoch 0112, val loss 4.5235, val loss 4.4787
Epoch 0113, val loss 4.4845, val loss 4.3852
Epoch 0114, val loss 4.4343, val loss 4.3553
Epoch 0115, val loss 4.3631, val loss 4.3061
Epoch 0116, val loss 4.3455, val loss 4.2671
Epoch 0117, val loss 4.4750, val loss 4.4251
Epoch 0118, val loss 4.3427, val loss 4.2338
Epoch 0119, val loss 4.3893, val loss 4.2824
Epoch 0120, val loss 4.4568, val loss 4.3277
Epoch 0121, val loss 4.4316, val loss 4.2983
Epoch 0122, val loss 4.3750, val loss 4.2460
Epoch 0123, val loss 4.2672, val loss 4.2261
Epoch 0124, val loss 4.4685, val loss 4.3630
Epoch 0125, val loss 4.3509, val loss 4.2226
Epoch 0126, val loss 4.2725, val loss 4.1633
Epoch 0127, val loss 4.3507, val loss 4.2161
Epoch 0128, val loss 4.3683, val loss 4.1961
Epoch 0129, val loss 4.2812, val loss 4.1729
Epoch 0130, val loss 4.3039, val loss 4.1883
Epoch 0131, val loss 4.3338, val loss 4.2248
Epoch 0132, val loss 4.2666, val loss 4.1256
Epoch 0133, val loss 4.1754, val loss 4.0774
Epoch 0134, val loss 4.3600, val loss 4.2093
Epoch 0135, val loss 4.3357, val loss 4.2407
Epoch 0136, val loss 4.3321, val loss 4.2022
Epoch 0137, val loss 4.3205, val loss 4.2756
Epoch 0138, val loss 4.2802, val loss 4.1563
Epoch 0139, val loss 4.2295, val loss 4.1007
Epoch 0140, val loss 4.2816, val loss 4.2175
Epoch 0141, val loss 4.2498, val loss 4.1315
Epoch 0142, val loss 4.2582, val loss 4.1930
Epoch 0143, val loss 4.1717, val loss 4.0970
Epoch 0144, val loss 4.1545, val loss 4.1109
Epoch 0145, val loss 4.1403, val loss 4.0242
Epoch 0146, val loss 4.2221, val loss 4.1098
Epoch 0147, val loss 4.2069, val loss 4.1465
Epoch 0148, val loss 4.2264, val loss 4.1265
Epoch 0149, val loss 4.2368, val loss 4.1396
Epoch 0150, val loss 4.2228, val loss 4.1082
Epoch 0151, val loss 4.1894, val loss 4.0301
Epoch 0152, val loss 4.0675, val loss 3.9920
Epoch 0153, val loss 4.1877, val loss 4.1360
Epoch 0154, val loss 4.1764, val loss 4.0769
Epoch 0155, val loss 4.0598, val loss 3.9425
Epoch 0156, val loss 4.0014, val loss 3.9383
Epoch 0157, val loss 4.1251, val loss 4.0324
Epoch 0158, val loss 4.1176, val loss 4.0293
Epoch 0159, val loss 4.0427, val loss 3.9379
Epoch 0160, val loss 4.0569, val loss 3.9513
Epoch 0161, val loss 4.0741, val loss 3.9590
Epoch 0162, val loss 4.1481, val loss 4.0610
Epoch 0163, val loss 4.0866, val loss 3.9952
Epoch 0164, val loss 4.1945, val loss 4.0976
Epoch 0165, val loss 4.2258, val loss 4.1207
Epoch 0166, val loss 4.2117, val loss 4.1210
Optimization Finished!
Total time elapsed: 2694.4000s
Loading 156th epoch
Test loss 3.9383

 #######################
 ##The 07th experiment##
 #######################
 learning rate: 0.0005
 Weight decay: 0.005
Construct model vgg13_bn
Epoch 0001, val loss 22.1620, val loss 21.5546
Epoch 0002, val loss 16.5998, val loss 16.5386
Epoch 0003, val loss 13.0940, val loss 13.1357
Epoch 0004, val loss 11.7840, val loss 11.8982
Epoch 0005, val loss 11.5060, val loss 11.6736
Epoch 0006, val loss 11.3014, val loss 11.3356
Epoch 0007, val loss 10.5288, val loss 10.5268
Epoch 0008, val loss 10.0422, val loss 10.1851
Epoch 0009, val loss 9.8553, val loss 9.9723
Epoch 0010, val loss 9.2700, val loss 9.4841
Epoch 0011, val loss 9.0142, val loss 9.0559
Epoch 0012, val loss 8.8484, val loss 8.8646
Epoch 0013, val loss 8.5884, val loss 8.6544
Epoch 0014, val loss 8.6339, val loss 8.7236
Epoch 0015, val loss 8.8222, val loss 8.9237
Epoch 0016, val loss 8.3377, val loss 8.3831
Epoch 0017, val loss 8.1763, val loss 8.2144
Epoch 0018, val loss 8.3717, val loss 8.4566
Epoch 0019, val loss 7.9180, val loss 7.9391
Epoch 0020, val loss 8.0943, val loss 8.1552
Epoch 0021, val loss 8.1127, val loss 8.1286
Epoch 0022, val loss 7.6396, val loss 7.6312
Epoch 0023, val loss 7.3673, val loss 7.4285
Epoch 0024, val loss 7.5446, val loss 7.5448
Epoch 0025, val loss 7.3661, val loss 7.4298
Epoch 0026, val loss 7.4034, val loss 7.4224
Epoch 0027, val loss 7.2633, val loss 7.1484
Epoch 0028, val loss 7.3279, val loss 7.3017
Epoch 0029, val loss 7.0413, val loss 7.0714
Epoch 0030, val loss 7.0731, val loss 7.1564
Epoch 0031, val loss 6.8214, val loss 6.8195
Epoch 0032, val loss 6.9714, val loss 7.0339
Epoch 0033, val loss 6.7477, val loss 6.7937
Epoch 0034, val loss 6.8887, val loss 6.8469
Epoch 0035, val loss 6.9045, val loss 6.9370
Epoch 0036, val loss 6.7598, val loss 6.7906
Epoch 0037, val loss 6.7420, val loss 6.7550
Epoch 0038, val loss 6.4784, val loss 6.4370
Epoch 0039, val loss 6.3910, val loss 6.3739
Epoch 0040, val loss 6.6954, val loss 6.6523
Epoch 0041, val loss 6.4264, val loss 6.4057
Epoch 0042, val loss 6.4871, val loss 6.4614
Epoch 0043, val loss 6.6635, val loss 6.5788
Epoch 0044, val loss 6.4284, val loss 6.3407
Epoch 0045, val loss 6.3188, val loss 6.3349
Epoch 0046, val loss 6.2201, val loss 6.1775
Epoch 0047, val loss 6.1781, val loss 6.0543
Epoch 0048, val loss 6.1228, val loss 6.1212
Epoch 0049, val loss 6.0125, val loss 5.9644
Epoch 0050, val loss 6.0698, val loss 6.0239
Epoch 0051, val loss 5.9509, val loss 5.8973
Epoch 0052, val loss 5.9412, val loss 5.9338
Epoch 0053, val loss 5.9660, val loss 5.9696
Epoch 0054, val loss 6.0663, val loss 6.0773
Epoch 0055, val loss 6.5548, val loss 6.4591
Epoch 0056, val loss 5.9808, val loss 5.9799
Epoch 0057, val loss 5.9751, val loss 5.9100
Epoch 0058, val loss 5.8027, val loss 5.7348
Epoch 0059, val loss 5.8998, val loss 5.8431
Epoch 0060, val loss 5.8601, val loss 5.7908
Epoch 0061, val loss 5.8370, val loss 5.8205
Epoch 0062, val loss 5.7209, val loss 5.7290
Epoch 0063, val loss 5.6199, val loss 5.5406
Epoch 0064, val loss 5.5532, val loss 5.5129
Epoch 0065, val loss 5.5668, val loss 5.5727
Epoch 0066, val loss 5.6403, val loss 5.6499
Epoch 0067, val loss 5.6283, val loss 5.5510
Epoch 0068, val loss 5.6509, val loss 5.5880
Epoch 0069, val loss 5.6235, val loss 5.5898
Epoch 0070, val loss 5.4882, val loss 5.4564
Epoch 0071, val loss 5.3658, val loss 5.3279
Epoch 0072, val loss 5.4054, val loss 5.3816
Epoch 0073, val loss 5.4590, val loss 5.3708
Epoch 0074, val loss 5.3772, val loss 5.3258
Epoch 0075, val loss 5.3552, val loss 5.3390
Epoch 0076, val loss 5.3692, val loss 5.3401
Epoch 0077, val loss 5.4980, val loss 5.5220
Epoch 0078, val loss 5.4768, val loss 5.4502
Epoch 0079, val loss 5.3130, val loss 5.3120
Epoch 0080, val loss 5.5388, val loss 5.4608
Epoch 0081, val loss 5.4885, val loss 5.4270
Epoch 0082, val loss 5.2194, val loss 5.2131
Epoch 0083, val loss 5.3761, val loss 5.2589
Epoch 0084, val loss 5.3732, val loss 5.3359
Epoch 0085, val loss 5.2822, val loss 5.2493
Epoch 0086, val loss 5.2899, val loss 5.2036
Epoch 0087, val loss 5.1864, val loss 5.1315
Epoch 0088, val loss 5.1256, val loss 5.1238
Epoch 0089, val loss 5.0846, val loss 4.9912
Epoch 0090, val loss 4.9853, val loss 4.9403
Epoch 0091, val loss 5.1358, val loss 5.0952
Epoch 0092, val loss 5.1469, val loss 5.0453
Epoch 0093, val loss 5.2094, val loss 5.2024
Epoch 0094, val loss 5.0765, val loss 5.0066
Epoch 0095, val loss 5.1854, val loss 5.0989
Epoch 0096, val loss 5.1021, val loss 5.0123
Epoch 0097, val loss 5.1232, val loss 5.0999
Epoch 0098, val loss 5.0083, val loss 4.9690
Epoch 0099, val loss 5.0354, val loss 4.9742
Epoch 0100, val loss 5.0762, val loss 5.0137
Optimization Finished!
Total time elapsed: 1624.6077s
Loading 90th epoch
Test loss 4.9403

 #######################
 ##The 08th experiment##
 #######################
 learning rate: 0.0005
 Weight decay: 0.001
Construct model vgg13_bn
Epoch 0001, val loss 22.1522, val loss 21.5120
Epoch 0002, val loss 16.2432, val loss 16.1497
Epoch 0003, val loss 12.9626, val loss 12.9712
Epoch 0004, val loss 11.6804, val loss 11.7183
Epoch 0005, val loss 11.1320, val loss 11.2664
Epoch 0006, val loss 10.5135, val loss 10.5791
Epoch 0007, val loss 10.3192, val loss 10.3613
Epoch 0008, val loss 9.5855, val loss 9.6464
Epoch 0009, val loss 9.7845, val loss 9.8228
Epoch 0010, val loss 9.4223, val loss 9.5462
Epoch 0011, val loss 9.1568, val loss 9.2640
Epoch 0012, val loss 9.2318, val loss 9.1588
Epoch 0013, val loss 9.0041, val loss 9.0067
Epoch 0014, val loss 8.4357, val loss 8.4890
Epoch 0015, val loss 8.6461, val loss 8.6994
Epoch 0016, val loss 8.4556, val loss 8.4923
Epoch 0017, val loss 8.1872, val loss 8.2956
Epoch 0018, val loss 7.9811, val loss 8.0590
Epoch 0019, val loss 8.0021, val loss 8.0263
Epoch 0020, val loss 7.7349, val loss 7.7743
Epoch 0021, val loss 7.9690, val loss 7.9533
Epoch 0022, val loss 7.5757, val loss 7.7061
Epoch 0023, val loss 7.6667, val loss 7.7839
Epoch 0024, val loss 7.4187, val loss 7.5314
Epoch 0025, val loss 7.2023, val loss 7.2530
Epoch 0026, val loss 7.2741, val loss 7.2847
Epoch 0027, val loss 7.4451, val loss 7.5657
Epoch 0028, val loss 7.1645, val loss 7.2698
Epoch 0029, val loss 6.9054, val loss 6.9596
Epoch 0030, val loss 6.9935, val loss 7.0425
Epoch 0031, val loss 6.8701, val loss 6.8750
Epoch 0032, val loss 6.9300, val loss 6.9400
Epoch 0033, val loss 6.6296, val loss 6.6506
Epoch 0034, val loss 6.6432, val loss 6.7018
Epoch 0035, val loss 6.8880, val loss 6.8986
Epoch 0036, val loss 6.6443, val loss 6.6720
Epoch 0037, val loss 6.5897, val loss 6.6397
Epoch 0038, val loss 6.5729, val loss 6.6083
Epoch 0039, val loss 6.4755, val loss 6.4428
Epoch 0040, val loss 6.3504, val loss 6.3916
Epoch 0041, val loss 6.2135, val loss 6.2338
Epoch 0042, val loss 6.2354, val loss 6.2457
Epoch 0043, val loss 6.1808, val loss 6.1529
Epoch 0044, val loss 5.9670, val loss 5.9972
Epoch 0045, val loss 6.1835, val loss 6.2203
Epoch 0046, val loss 6.2636, val loss 6.2678
Epoch 0047, val loss 6.2167, val loss 6.2008
Epoch 0048, val loss 6.1972, val loss 6.1896
Epoch 0049, val loss 6.0932, val loss 6.0568
Epoch 0050, val loss 6.0040, val loss 6.0138
Epoch 0051, val loss 5.8417, val loss 5.8616
Epoch 0052, val loss 5.8959, val loss 5.8942
Epoch 0053, val loss 5.8133, val loss 5.8464
Epoch 0054, val loss 5.9956, val loss 5.9818
Epoch 0055, val loss 5.8221, val loss 5.8046
Epoch 0056, val loss 5.5899, val loss 5.6085
Epoch 0057, val loss 5.7928, val loss 5.8039
Epoch 0058, val loss 5.8744, val loss 5.9527
Epoch 0059, val loss 5.7136, val loss 5.6494
Epoch 0060, val loss 5.7502, val loss 5.7715
Epoch 0061, val loss 5.8650, val loss 5.8717
Epoch 0062, val loss 5.6732, val loss 5.6668
Epoch 0063, val loss 5.8381, val loss 5.8372
Epoch 0064, val loss 5.6218, val loss 5.5730
Epoch 0065, val loss 5.5963, val loss 5.5501
Epoch 0066, val loss 5.7808, val loss 5.7648
Optimization Finished!
Total time elapsed: 1071.1392s
Loading 56th epoch
Test loss 5.6085

 #######################
 ##The 09th experiment##
 #######################
 learning rate: 0.0001
 Weight decay: 0.005
Construct model vgg13_bn
Epoch 0001, val loss 25.9217, val loss 25.3824
Epoch 0002, val loss 25.6276, val loss 25.1954
Epoch 0003, val loss 23.5977, val loss 23.2059
Epoch 0004, val loss 19.7824, val loss 19.7736
Epoch 0005, val loss 17.1753, val loss 17.2373
Epoch 0006, val loss 14.5629, val loss 14.7545
Epoch 0007, val loss 12.6046, val loss 12.7746
Epoch 0008, val loss 11.9137, val loss 12.0908
Epoch 0009, val loss 11.3459, val loss 11.4465
Epoch 0010, val loss 11.0347, val loss 11.2100
Epoch 0011, val loss 10.6531, val loss 10.7943
Epoch 0012, val loss 10.3072, val loss 10.4665
Epoch 0013, val loss 9.9999, val loss 10.1409
Epoch 0014, val loss 9.9149, val loss 10.0579
Epoch 0015, val loss 9.5835, val loss 9.7386
Epoch 0016, val loss 9.5761, val loss 9.7294
Epoch 0017, val loss 9.3172, val loss 9.4426
Epoch 0018, val loss 9.3184, val loss 9.4965
Epoch 0019, val loss 9.1012, val loss 9.2945
Epoch 0020, val loss 9.1171, val loss 9.2721
Epoch 0021, val loss 8.9773, val loss 9.1532
Epoch 0022, val loss 8.8956, val loss 9.0376
Epoch 0023, val loss 8.7579, val loss 8.8867
Epoch 0024, val loss 8.6720, val loss 8.8652
Epoch 0025, val loss 8.4220, val loss 8.6012
Epoch 0026, val loss 8.5748, val loss 8.7949
Epoch 0027, val loss 8.2954, val loss 8.4874
Epoch 0028, val loss 8.4244, val loss 8.6309
Epoch 0029, val loss 8.2404, val loss 8.3982
Epoch 0030, val loss 8.3502, val loss 8.4564
Epoch 0031, val loss 8.2658, val loss 8.3981
Epoch 0032, val loss 8.0616, val loss 8.2059
Epoch 0033, val loss 8.2666, val loss 8.3874
Epoch 0034, val loss 7.9503, val loss 8.0863
Epoch 0035, val loss 8.2307, val loss 8.3347
Epoch 0036, val loss 8.0878, val loss 8.2025
Epoch 0037, val loss 8.0057, val loss 8.0985
Epoch 0038, val loss 7.7874, val loss 7.9092
Epoch 0039, val loss 7.8813, val loss 7.9856
Epoch 0040, val loss 7.7371, val loss 7.8448
Epoch 0041, val loss 7.7758, val loss 7.8455
Epoch 0042, val loss 7.7231, val loss 7.8106
Epoch 0043, val loss 7.7133, val loss 7.8200
Epoch 0044, val loss 7.6669, val loss 7.7734
Epoch 0045, val loss 7.5556, val loss 7.6692
Epoch 0046, val loss 7.6228, val loss 7.6907
Epoch 0047, val loss 7.4858, val loss 7.5641
Epoch 0048, val loss 7.4167, val loss 7.5053
Epoch 0049, val loss 7.4615, val loss 7.5519
Epoch 0050, val loss 7.3808, val loss 7.4543
Epoch 0051, val loss 7.4548, val loss 7.5414
Epoch 0052, val loss 7.4859, val loss 7.5552
Epoch 0053, val loss 7.2956, val loss 7.3800
Epoch 0054, val loss 7.2294, val loss 7.3177
Epoch 0055, val loss 7.4109, val loss 7.4972
Epoch 0056, val loss 7.4705, val loss 7.4924
Epoch 0057, val loss 7.1872, val loss 7.2875
Epoch 0058, val loss 7.2682, val loss 7.3335
Epoch 0059, val loss 7.0553, val loss 7.1623
Epoch 0060, val loss 7.1647, val loss 7.2410
Epoch 0061, val loss 7.2130, val loss 7.3350
Epoch 0062, val loss 7.0733, val loss 7.2060
Epoch 0063, val loss 7.0358, val loss 7.1216
Epoch 0064, val loss 7.0577, val loss 7.0792
Epoch 0065, val loss 7.1410, val loss 7.2278
Epoch 0066, val loss 7.0500, val loss 7.0866
Epoch 0067, val loss 7.1369, val loss 7.1664
Epoch 0068, val loss 7.1031, val loss 7.2290
Epoch 0069, val loss 6.9004, val loss 6.9421
Epoch 0070, val loss 6.9651, val loss 7.0131
Epoch 0071, val loss 6.8239, val loss 6.8422
Epoch 0072, val loss 6.8176, val loss 6.8497
Epoch 0073, val loss 6.8544, val loss 6.9309
Epoch 0074, val loss 6.7835, val loss 6.8750
Epoch 0075, val loss 6.7875, val loss 6.8030
Epoch 0076, val loss 6.8700, val loss 6.8537
Epoch 0077, val loss 6.7531, val loss 6.8062
Epoch 0078, val loss 6.8018, val loss 6.8469
Epoch 0079, val loss 6.6542, val loss 6.7272
Epoch 0080, val loss 6.6912, val loss 6.7642
Epoch 0081, val loss 6.6747, val loss 6.7166
Epoch 0082, val loss 6.5338, val loss 6.6265
Epoch 0083, val loss 6.5197, val loss 6.5205
Epoch 0084, val loss 6.7279, val loss 6.7628
Epoch 0085, val loss 6.7601, val loss 6.8322
Epoch 0086, val loss 6.5302, val loss 6.5312
Epoch 0087, val loss 6.4417, val loss 6.4752
Epoch 0088, val loss 6.5808, val loss 6.6219
Epoch 0089, val loss 6.5620, val loss 6.5912
Epoch 0090, val loss 6.4907, val loss 6.5333
Epoch 0091, val loss 6.5302, val loss 6.6108
Epoch 0092, val loss 6.3153, val loss 6.3709
Epoch 0093, val loss 6.3744, val loss 6.4179
Epoch 0094, val loss 6.2190, val loss 6.2720
Epoch 0095, val loss 6.3148, val loss 6.3155
Epoch 0096, val loss 6.2510, val loss 6.3027
Epoch 0097, val loss 6.3218, val loss 6.3920
Epoch 0098, val loss 6.3661, val loss 6.4568
Epoch 0099, val loss 6.2417, val loss 6.2770
Epoch 0100, val loss 6.2347, val loss 6.2840
Epoch 0101, val loss 6.1306, val loss 6.2144
Epoch 0102, val loss 6.3037, val loss 6.3033
Epoch 0103, val loss 6.2335, val loss 6.2218
Epoch 0104, val loss 6.1217, val loss 6.1710
Epoch 0105, val loss 6.1691, val loss 6.2042
Epoch 0106, val loss 6.0375, val loss 6.1101
Epoch 0107, val loss 6.1006, val loss 6.1284
Epoch 0108, val loss 6.2641, val loss 6.2998
Epoch 0109, val loss 6.0790, val loss 6.0827
Epoch 0110, val loss 6.0612, val loss 6.0995
Epoch 0111, val loss 6.0650, val loss 6.1021
Epoch 0112, val loss 6.0428, val loss 6.0795
Epoch 0113, val loss 6.0278, val loss 6.0591
Epoch 0114, val loss 6.1117, val loss 6.0917
Epoch 0115, val loss 6.1038, val loss 6.1728
Epoch 0116, val loss 6.0882, val loss 6.0685
Epoch 0117, val loss 6.0298, val loss 6.0679
Epoch 0118, val loss 5.9712, val loss 5.9985
Epoch 0119, val loss 5.9153, val loss 5.9459
Epoch 0120, val loss 6.0360, val loss 6.0680
Epoch 0121, val loss 5.9286, val loss 5.9799
Epoch 0122, val loss 6.1565, val loss 6.1542
Epoch 0123, val loss 5.9762, val loss 5.9999
Epoch 0124, val loss 6.0583, val loss 6.0807
Epoch 0125, val loss 6.0654, val loss 6.0356
Epoch 0126, val loss 5.8997, val loss 5.8549
Epoch 0127, val loss 5.9492, val loss 5.9267
Epoch 0128, val loss 5.9383, val loss 5.9410
Epoch 0129, val loss 6.0509, val loss 6.0412
Epoch 0130, val loss 6.0469, val loss 6.0133
Epoch 0131, val loss 5.9099, val loss 5.9205
Epoch 0132, val loss 5.7299, val loss 5.7538
Epoch 0133, val loss 5.8558, val loss 5.8206
Epoch 0134, val loss 5.8894, val loss 5.8299
Epoch 0135, val loss 5.7554, val loss 5.7707
Epoch 0136, val loss 5.7789, val loss 5.7791
Epoch 0137, val loss 5.8157, val loss 5.7191
Epoch 0138, val loss 5.8511, val loss 5.8108
Epoch 0139, val loss 5.7359, val loss 5.7492
Epoch 0140, val loss 5.7365, val loss 5.6969
Epoch 0141, val loss 5.6951, val loss 5.6477
Epoch 0142, val loss 5.7442, val loss 5.7279
Epoch 0143, val loss 5.7763, val loss 5.7514
Epoch 0144, val loss 5.6659, val loss 5.6843
Epoch 0145, val loss 5.6973, val loss 5.6759
Epoch 0146, val loss 5.6692, val loss 5.7101
Epoch 0147, val loss 5.6873, val loss 5.7051
Epoch 0148, val loss 5.7205, val loss 5.7716
Epoch 0149, val loss 5.6111, val loss 5.5883
Epoch 0150, val loss 5.6132, val loss 5.6104
Epoch 0151, val loss 5.7097, val loss 5.7097
Epoch 0152, val loss 5.6346, val loss 5.6347
Epoch 0153, val loss 5.6526, val loss 5.5688
Epoch 0154, val loss 5.6021, val loss 5.5979
Epoch 0155, val loss 5.4714, val loss 5.4923
Epoch 0156, val loss 5.4845, val loss 5.4853
Epoch 0157, val loss 5.6524, val loss 5.6465
Epoch 0158, val loss 5.5471, val loss 5.5253
Epoch 0159, val loss 5.5619, val loss 5.5657
Epoch 0160, val loss 5.4932, val loss 5.5033
Epoch 0161, val loss 5.6112, val loss 5.6213
Epoch 0162, val loss 5.5698, val loss 5.5540
Epoch 0163, val loss 5.5031, val loss 5.5337
Epoch 0164, val loss 5.5250, val loss 5.5134
Epoch 0165, val loss 5.4444, val loss 5.4130
Epoch 0166, val loss 5.4977, val loss 5.5390
Epoch 0167, val loss 5.5110, val loss 5.5010
Epoch 0168, val loss 5.4825, val loss 5.4227
Epoch 0169, val loss 5.4130, val loss 5.4613
Epoch 0170, val loss 5.5117, val loss 5.4646
Epoch 0171, val loss 5.4299, val loss 5.3664
Epoch 0172, val loss 5.4186, val loss 5.4580
Epoch 0173, val loss 5.4190, val loss 5.4638
Epoch 0174, val loss 5.4295, val loss 5.3664
Epoch 0175, val loss 5.4807, val loss 5.4254
Epoch 0176, val loss 5.4169, val loss 5.4324
Epoch 0177, val loss 5.4295, val loss 5.3764
Epoch 0178, val loss 5.4886, val loss 5.3911
Epoch 0179, val loss 5.4004, val loss 5.3549
Epoch 0180, val loss 5.3749, val loss 5.3646
Epoch 0181, val loss 5.3907, val loss 5.3278
Epoch 0182, val loss 5.3719, val loss 5.4030
Epoch 0183, val loss 5.4521, val loss 5.4204
Epoch 0184, val loss 5.3092, val loss 5.3481
Epoch 0185, val loss 5.3501, val loss 5.3518
Epoch 0186, val loss 5.2587, val loss 5.1997
Epoch 0187, val loss 5.3853, val loss 5.3251
Epoch 0188, val loss 5.2738, val loss 5.2717
Epoch 0189, val loss 5.2353, val loss 5.2071
Epoch 0190, val loss 5.3196, val loss 5.2972
Epoch 0191, val loss 5.2380, val loss 5.2155
Epoch 0192, val loss 5.2576, val loss 5.1930
Epoch 0193, val loss 5.1918, val loss 5.1621
Epoch 0194, val loss 5.2436, val loss 5.2002
Epoch 0195, val loss 5.2351, val loss 5.2597
Epoch 0196, val loss 5.2153, val loss 5.2259
Epoch 0197, val loss 5.1659, val loss 5.1505
Epoch 0198, val loss 5.1853, val loss 5.1859
Epoch 0199, val loss 5.1881, val loss 5.1596
Epoch 0200, val loss 5.1825, val loss 5.1926
Epoch 0201, val loss 5.1659, val loss 5.1833
Epoch 0202, val loss 5.0889, val loss 5.1124
Epoch 0203, val loss 5.3537, val loss 5.3970
Epoch 0204, val loss 5.2120, val loss 5.2274
Epoch 0205, val loss 5.1145, val loss 5.1052
Epoch 0206, val loss 5.2287, val loss 5.2080
Epoch 0207, val loss 5.1489, val loss 5.1602
Epoch 0208, val loss 5.1578, val loss 5.1505
Epoch 0209, val loss 5.1781, val loss 5.1929
Epoch 0210, val loss 5.2202, val loss 5.2063
Epoch 0211, val loss 5.1544, val loss 5.1019
Epoch 0212, val loss 5.2246, val loss 5.1795
Optimization Finished!
Total time elapsed: 3443.5865s
Loading 202th epoch
Test loss 5.1124

 #######################
 ##The 10th experiment##
 #######################
 learning rate: 0.0001
 Weight decay: 0.001
Construct model vgg13_bn
Epoch 0001, val loss 25.9530, val loss 25.4134
Epoch 0002, val loss 25.6433, val loss 25.2126
Epoch 0003, val loss 23.6385, val loss 23.2380
Epoch 0004, val loss 19.7291, val loss 19.7538
Epoch 0005, val loss 17.1194, val loss 17.1796
Epoch 0006, val loss 14.5208, val loss 14.6462
Epoch 0007, val loss 12.7420, val loss 12.8748
Epoch 0008, val loss 11.8437, val loss 11.9736
Epoch 0009, val loss 11.4390, val loss 11.4919
Epoch 0010, val loss 10.6431, val loss 10.7771
Epoch 0011, val loss 10.9281, val loss 11.0519
Epoch 0012, val loss 10.0686, val loss 10.2177
Epoch 0013, val loss 9.9130, val loss 10.0478
Epoch 0014, val loss 9.6872, val loss 9.7752
Epoch 0015, val loss 9.7305, val loss 9.8319
Epoch 0016, val loss 9.6296, val loss 9.7324
Epoch 0017, val loss 9.2774, val loss 9.4487
Epoch 0018, val loss 9.3713, val loss 9.4437
Epoch 0019, val loss 9.3455, val loss 9.4440
Epoch 0020, val loss 9.3892, val loss 9.4668
Epoch 0021, val loss 8.8576, val loss 8.9458
Epoch 0022, val loss 8.8035, val loss 8.9538
Epoch 0023, val loss 8.7079, val loss 8.8589
Epoch 0024, val loss 8.6226, val loss 8.7126
Epoch 0025, val loss 8.7450, val loss 8.8634
Epoch 0026, val loss 8.5102, val loss 8.6538
Epoch 0027, val loss 8.5458, val loss 8.7218
Epoch 0028, val loss 8.6167, val loss 8.6282
Epoch 0029, val loss 8.3654, val loss 8.4891
Epoch 0030, val loss 8.0498, val loss 8.1584
Epoch 0031, val loss 8.3267, val loss 8.4420
Epoch 0032, val loss 8.2261, val loss 8.3586
Epoch 0033, val loss 8.1774, val loss 8.2818
Epoch 0034, val loss 8.0437, val loss 8.1477
Epoch 0035, val loss 7.9593, val loss 8.0786
Epoch 0036, val loss 7.9289, val loss 8.0586
Epoch 0037, val loss 7.9586, val loss 7.9856
Epoch 0038, val loss 7.9513, val loss 7.9962
Epoch 0039, val loss 7.7933, val loss 7.9028
Epoch 0040, val loss 7.7995, val loss 7.9048
Epoch 0041, val loss 7.7464, val loss 7.8254
Epoch 0042, val loss 7.6778, val loss 7.7850
Epoch 0043, val loss 7.5819, val loss 7.6516
Epoch 0044, val loss 7.8586, val loss 7.9035
Epoch 0045, val loss 7.7434, val loss 7.8023
Epoch 0046, val loss 7.5334, val loss 7.6312
Epoch 0047, val loss 7.4816, val loss 7.5255
Epoch 0048, val loss 7.5731, val loss 7.5662
Epoch 0049, val loss 7.5162, val loss 7.5924
Epoch 0050, val loss 7.3445, val loss 7.3891
Epoch 0051, val loss 7.4733, val loss 7.5534
Epoch 0052, val loss 7.4854, val loss 7.5929
Epoch 0053, val loss 7.3827, val loss 7.4420
Epoch 0054, val loss 7.4640, val loss 7.5346
Epoch 0055, val loss 7.1882, val loss 7.2440
Epoch 0056, val loss 7.3031, val loss 7.3346
Epoch 0057, val loss 7.0176, val loss 7.1027
Epoch 0058, val loss 7.0158, val loss 7.0543
Epoch 0059, val loss 7.0510, val loss 7.0832
Epoch 0060, val loss 7.1080, val loss 7.1311
Epoch 0061, val loss 7.4167, val loss 7.4489
Epoch 0062, val loss 7.1238, val loss 7.1449
Epoch 0063, val loss 7.1261, val loss 7.1520
Epoch 0064, val loss 7.2080, val loss 7.2380
Epoch 0065, val loss 6.8984, val loss 6.9208
Epoch 0066, val loss 6.7907, val loss 6.8108
Epoch 0067, val loss 6.7634, val loss 6.8388
Epoch 0068, val loss 6.8523, val loss 6.8858
Epoch 0069, val loss 6.7587, val loss 6.7647
Epoch 0070, val loss 6.8899, val loss 6.8980
Epoch 0071, val loss 6.7675, val loss 6.7714
Epoch 0072, val loss 6.7954, val loss 6.8210
Epoch 0073, val loss 6.7826, val loss 6.7631
Epoch 0074, val loss 6.8759, val loss 6.8687
Epoch 0075, val loss 6.8462, val loss 6.8264
Epoch 0076, val loss 6.6506, val loss 6.7006
Epoch 0077, val loss 6.6690, val loss 6.7119
Epoch 0078, val loss 6.7703, val loss 6.8230
Epoch 0079, val loss 6.6841, val loss 6.7081
Epoch 0080, val loss 6.6421, val loss 6.6987
Epoch 0081, val loss 6.5812, val loss 6.5531
Epoch 0082, val loss 6.5493, val loss 6.5731
Epoch 0083, val loss 6.6014, val loss 6.6411
Epoch 0084, val loss 6.4768, val loss 6.5656
Epoch 0085, val loss 6.4209, val loss 6.4346
Epoch 0086, val loss 6.4981, val loss 6.5408
Epoch 0087, val loss 6.5505, val loss 6.5693
Epoch 0088, val loss 6.3913, val loss 6.4209
Epoch 0089, val loss 6.4709, val loss 6.4992
Epoch 0090, val loss 6.4989, val loss 6.4770
Epoch 0091, val loss 6.4184, val loss 6.4378
Epoch 0092, val loss 6.3843, val loss 6.4064
Epoch 0093, val loss 6.3218, val loss 6.3787
Epoch 0094, val loss 6.3954, val loss 6.4360
Epoch 0095, val loss 6.3766, val loss 6.4939
Epoch 0096, val loss 6.3846, val loss 6.4318
Epoch 0097, val loss 6.2009, val loss 6.1949
Epoch 0098, val loss 6.4112, val loss 6.4364
Epoch 0099, val loss 6.3073, val loss 6.2739
Epoch 0100, val loss 6.2838, val loss 6.3107
Epoch 0101, val loss 6.2987, val loss 6.3288
Epoch 0102, val loss 6.3032, val loss 6.3098
Epoch 0103, val loss 6.3360, val loss 6.3349
Epoch 0104, val loss 6.2637, val loss 6.2213
Epoch 0105, val loss 6.3435, val loss 6.2695
Epoch 0106, val loss 6.1646, val loss 6.1907
Epoch 0107, val loss 6.2876, val loss 6.3365
Epoch 0108, val loss 6.1412, val loss 6.1658
Epoch 0109, val loss 6.1438, val loss 6.1908
Epoch 0110, val loss 6.1791, val loss 6.1837
Epoch 0111, val loss 6.1126, val loss 6.0838
Epoch 0112, val loss 5.9964, val loss 6.0063
Epoch 0113, val loss 6.1944, val loss 6.1780
Epoch 0114, val loss 6.0875, val loss 6.1134
Epoch 0115, val loss 6.1115, val loss 6.1464
Epoch 0116, val loss 6.0529, val loss 6.0414
Epoch 0117, val loss 6.0009, val loss 6.0104
Epoch 0118, val loss 6.0140, val loss 6.0501
Epoch 0119, val loss 5.9787, val loss 5.9556
Epoch 0120, val loss 6.0449, val loss 6.0458
Epoch 0121, val loss 5.9676, val loss 5.9900
Epoch 0122, val loss 5.9484, val loss 5.8922
Epoch 0123, val loss 5.9931, val loss 5.9787
Epoch 0124, val loss 5.8962, val loss 5.8465
Epoch 0125, val loss 5.8902, val loss 5.8897
Epoch 0126, val loss 6.0769, val loss 6.0651
Epoch 0127, val loss 5.8473, val loss 5.8362
Epoch 0128, val loss 5.9598, val loss 5.9280
Epoch 0129, val loss 5.8383, val loss 5.8357
Epoch 0130, val loss 5.8976, val loss 5.8621
Epoch 0131, val loss 5.9088, val loss 5.8975
Epoch 0132, val loss 5.8144, val loss 5.8174
Epoch 0133, val loss 5.7636, val loss 5.7178
Epoch 0134, val loss 5.9731, val loss 5.9547
Epoch 0135, val loss 5.9469, val loss 5.9105
Epoch 0136, val loss 5.9007, val loss 5.8621
Epoch 0137, val loss 5.9101, val loss 5.8859
Epoch 0138, val loss 5.8626, val loss 5.8589
Epoch 0139, val loss 5.7679, val loss 5.7605
Epoch 0140, val loss 5.7680, val loss 5.7404
Epoch 0141, val loss 5.7862, val loss 5.8045
Epoch 0142, val loss 5.7747, val loss 5.7587
Epoch 0143, val loss 5.8631, val loss 5.8965
Optimization Finished!
Total time elapsed: 2321.6021s
Loading 133th epoch
Test loss 5.7178

